# Fetches reviews from goodreads
import json
import sys
import re
import requests
from bs4 import BeautifulSoup
import json
import re
from whoswho import who
import time
from selenium import webdriver
from selenium.common.exceptions import JavascriptException, ElementNotInteractableException, ElementClickInterceptedException
import re

RATING_STARS_DICT = {'it was amazing': 5,
                     'really liked it': 4,
                     'liked it': 3,
                     'it was ok': 2,
                     'did not like it': 1,
                     '': None}


def handle_book_title(book_title):
    book_title = book_title.replace("\r", " ")
    book_title = book_title.replace("\t", " ")
    book_title = book_title.replace("\n", " ")
    return book_title


def author_match(author_name, authors):
    for author in authors:
        if who.match(author_name, author) or author_name == author:
            return True
    return False


def fetch_review(book_title, author_name):
    book_title = handle_book_title(book_title)
    r = requests.get(
        f"https://www.goodreads.com/search?q={book_title.replace(' ', '+')}")
    soup = BeautifulSoup(r.text, 'html.parser')

    table = soup.find('table', class_='tableList')

    # Some book titles might have a unnoficial subtitle, depending on the book version
    # Even with different subtitles, the books might be the same
    # Since the query for such book might fail due to the subtitle, we try to remove it

    if (not table):
        new_book_title = book_title.split(':')[0]
        print(
            f"WARNING: No review found for '{book_title}'. Trying '{new_book_title}'", file=sys.stderr)
        r = requests.get(
            f"https://www.goodreads.com/search?q={new_book_title.replace(' ', '+')}")
        soup = BeautifulSoup(r.text, 'html.parser')

        table = soup.find('table', class_='tableList')

        if (not table):
            print(
                f"WARNING: No review found for {new_book_title}.", file=sys.stderr)
            return None

    rows = table.find_all('tr')
    targetColumns = [row.find_all('td')[1] for row in rows]
    authors = [[authorLink.find('span').text for authorLink in column.find_all(
        'a', class_='authorName')] for column in targetColumns]

    hyperlinks = [targetColumn.find('a', class_='bookTitle')[
        'href'] for idx, targetColumn in enumerate(targetColumns) if author_match(author_name, authors[idx])]

    if (hyperlinks != []):
        return hyperlinks[0]

    return None


def switch_reviews_mode(driver, book_id, sort_order, rating=None):
    """
    Copyright (C) 2019 by Omar Einea: https://github.com/OmarEinea/GoodReadsScraper
    Licensed under GPL v3.0: https://github.com/OmarEinea/GoodReadsScraper/blob/master/LICENSE.md
    Accessed on 2019-12-01.
    """
    edition_reviews = False
    rating = str(rating) if rating else ''
    driver.execute_script(
        'document.getElementById("reviews").insertAdjacentHTML("beforeend", \'<a data-remote="true" rel="nofollow"'
        f'class="actionLinkLite loadingLink" data-keep-on-success="true" id="switch{rating}{sort_order}"' +
        f'href="/book/reviews/{book_id}?rating={rating}&sort={sort_order}' +
        ('&edition_reviews=true' if edition_reviews else '') + '">Switch Mode</a>\');' +
        f'document.getElementById("switch{rating}{sort_order}").click()'
    )
    return True


def get_rating(node):
    if len(node.find_all('span', {'class': 'staticStars'})) > 0:
        rating = node.find_all('span', {'class': 'staticStars'})[0]['title']
        return RATING_STARS_DICT[rating]
    return ''


def get_user_name(node):
    if len(node.find_all('a', {'class': 'user'})) > 0:
        return node.find_all('a', {'class': 'user'})[0]['title']
    return ''


def get_user_url(node):
    if len(node.find_all('a', {'class': 'user'})) > 0:
        return node.find_all('a', {'class': 'user'})[0]['href']
    return ''


def get_date(node):
    if len(node.find_all('a', {'class': 'reviewDate createdAt right'})) > 0:
        return node.find_all('a', {'class': 'reviewDate createdAt right'})[0].text
    return ''


def get_text(node):

    display_text = ''
    full_text = ''

    if len(node.find_all('span', {'class': 'readable'})) > 0:
        for child in node.find_all('span', {'class': 'readable'})[0].children:
            if child.name == 'span' and 'style' not in child:
                display_text = child.text
            if child.name == 'span' and 'style' in child and child['style'] == 'display:none':
                full_text = child.text

    if full_text:
        return full_text

    return display_text


def get_num_likes(node):
    if node.find('span', {'class': 'likesCount'}) and len(node.find('span', {'class': 'likesCount'})) > 0:
        likes = node.find('span', {'class': 'likesCount'}).text
        if 'likes' in likes:
            return int(likes.split()[0])
    return 0


def get_shelves(node):
    shelves = []
    if node.find('div', {'class': 'uitext greyText bookshelves'}):
        _shelves_node = node.find(
            'div', {'class': 'uitext greyText bookshelves'})
        for _shelf_node in _shelves_node.find_all('a'):
            shelves.append(_shelf_node.text)
    return shelves


def get_id(bookid):
    pattern = re.compile("([^.]+)")
    return pattern.search(bookid).group()


def fetch_review_info(book_id, num_reviews=10, num_attempts=15):

    driver = webdriver.Firefox(executable_path="C:\\Users\\up201906272\\Desktop\\geckodriver.exe")
    url = 'https://www.goodreads.com/book/show/' + book_id
    driver.get(url)
    reviews = []
    
    if num_attempts == 0: return reviews

    try:
        time.sleep(4)

        switch_reviews_mode(driver, book_id, 'default')
        time.sleep(2)

        # Pull the page source, load into BeautifulSoup, and find all review nodes.
        source = driver.page_source
        soup = BeautifulSoup(source, 'lxml')
        nodes = soup.find_all('div', {'class': 'review'})
        book_title = soup.find(id='bookTitle').text.strip()

        # Iterate through and parse the reviews.
        for node in nodes:
            review_id = re.search('[0-9]+', node['id']).group(0)
            reviews.append({'date': get_date(node),
                            'rating': get_rating(node),
                            'user_name': get_user_name(node),
                            'text': get_text(node),
                            'num_likes': get_num_likes(node)})
            num_reviews = num_reviews - 1
            if (num_reviews <= 0):
                break

    except ElementClickInterceptedException:
        print(f'ðŸš¨ ElementClickInterceptedException (Likely a pop-up)ðŸš¨\nðŸ”„ Refreshing Goodreads site and rescraping bookðŸ”„', file=sys.stderr)
        reviews = fetch_review_info(book_id, num_reviews, num_attempts - 1)

    except ElementNotInteractableException:
        print('ðŸš¨ ElementNotInteractableExceptionðŸš¨ \nðŸ”„ Refreshing Goodreads site and rescraping bookðŸ”„', file=sys.stderr)
        reviews = fetch_review_info(book_id, num_reviews, num_attempts - 1)

    except JavascriptException:
        print('ðŸš¨ JavascriptException \nðŸ”„ Refreshing Goodreads site and rescraping bookðŸ”„', file=sys.stderr)
        reviews = fetch_review_info(book_id, num_reviews, num_attempts - 1)

    except:
        print('ðŸš¨ UnknownException \nðŸ”„ Refreshing Goodreads site and rescraping bookðŸ”„', file=sys.stderr)
        reviews = fetch_review_info(book_id, num_reviews, num_attempts - 1)

    driver.close()
    return reviews


def retrieve_book_id(book_review_link):
    print(book_review_link, file=sys.stderr)
    match = re.search('/book/show/(\d+)', book_review_link)
    if match:
        return match.group(1)
    return None


def save_review(folder, book_info, book_id):
    with open(f"output/{folder}/{book_id}.json", 'w') as f:
        json.dump(book_info, f, indent=4)

for idx, book in enumerate(sys.stdin):
    book = json.loads(book.strip())
    print(f"{idx + 1} Fetching reviews for: {book['id']}", file=sys.stderr)
    
    first_author = (book['authors'][0]['first_name'] or '') + \
        ' ' + (book['authors'][0]['last_name'] or '')

    book_review_link = fetch_review(
        book['title'], first_author.strip() or '')
    book_id = retrieve_book_id(book_review_link)

    if (not book_id):
        print("WARNING: No book ID found", file=sys.stderr)
        continue

    print(f"{book['id']},{book_id}")
